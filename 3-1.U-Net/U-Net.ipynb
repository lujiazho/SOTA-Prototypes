{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coded by Lujia Zhong @lujiazho<br>\n",
    "Reference: https://github.com/milesial/Pytorch-UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight map described in [U-Net](https://arxiv.org/pdf/1505.04597) is not adapted here, because it's pre-computed w.r.t cell segmentation labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DownSampling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0, bias=False)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0, bias=False)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[-2] % 2 == 0 and x.shape[-1] % 2 == 0, \"Both H and W must be even.\"\n",
    "        \n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class UpSampling(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # ConvTranspose2d has trainable kernels; UpSampling2D is interpolation: bilinear/nearest...\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0, bias=False)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0, bias=False)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        pad_h, pad_w = (x2.shape[-2]-x1.shape[-2])//2, (x2.shape[-1]-x1.shape[-1])//2\n",
    "        x = torch.cat([x2[:,:,pad_h:-pad_h,pad_w:-pad_w], x1], dim=1)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.in_conv = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=0, bias=False),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=0, bias=False),\n",
    "        )\n",
    "        \n",
    "        self.down1 = DownSampling(64, 128)\n",
    "        self.down2 = DownSampling(128, 256)\n",
    "        self.down3 = DownSampling(256, 512)\n",
    "        self.down4 = DownSampling(512, 1024)\n",
    "        \n",
    "        self.up1 = UpSampling(1024, 512)\n",
    "        self.up2 = UpSampling(512, 256)\n",
    "        self.up3 = UpSampling(256, 128)\n",
    "        self.up4 = UpSampling(128, 64)\n",
    "        \n",
    "        self.out_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # torch.Size([3, 776, 776])\n",
    "        \n",
    "        x = self.preprocess(x) # torch.Size([4, 3, 572, 572])\n",
    "        \n",
    "        x1 = self.in_conv(x)   # torch.Size([4, 64, 568, 568])\n",
    "        \n",
    "        x2 = self.down1(x1)    # torch.Size([4, 128, 280, 280])\n",
    "        x3 = self.down2(x2)    # torch.Size([4, 256, 136, 136])\n",
    "        x4 = self.down3(x3)    # torch.Size([4, 512, 64, 64])\n",
    "        x5 = self.down4(x4)    # torch.Size([4, 1024, 28, 28])\n",
    "        \n",
    "        x = self.up1(x5, x4)   # torch.Size([4, 512, 52, 52])\n",
    "        x = self.up2(x, x3)    # torch.Size([4, 256, 100, 100])\n",
    "        x = self.up3(x, x2)    # torch.Size([4, 128, 196, 196])\n",
    "        x = self.up4(x, x1)    # torch.Size([4, 64, 388, 388])\n",
    "        \n",
    "        x = self.out_conv(x)   # torch.Size([4, 10, 388, 388])\n",
    "        \n",
    "        logits = self.postprocess(x)   # torch.Size([10, 776, 776])\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    # pad and slice into patches\n",
    "    def preprocess(self, x, slice_ = 2):\n",
    "        ch, H, W = x.shape\n",
    "        assert H % slice_ == 0 and W % slice_ == 0, \"Cannot split imgs.\"\n",
    "        \n",
    "        pad = 92   # as in paper\n",
    "        new_H, new_W = H // slice_, W // slice_\n",
    "        \n",
    "        x = nn.ReflectionPad2d(pad)(x)\n",
    "        \n",
    "        imgs = []\n",
    "        for i in range(pad, H+pad, new_H):\n",
    "            for j in range(pad, W+pad, new_W):\n",
    "                imgs.append(x[:,i-pad:i+new_H+pad,j-pad:j+new_W+pad])\n",
    "                \n",
    "        x = torch.stack(imgs)\n",
    "        return x\n",
    "    \n",
    "    def postprocess(self, x):\n",
    "        patch_num, ch, H, W = x.shape\n",
    "        n = int(patch_num**0.5)\n",
    "                \n",
    "        x = x.view(n, n, ch, H, W)\n",
    "        x = x.permute(2, 0, 3, 1, 4).contiguous()\n",
    "                \n",
    "        return x.view(ch, H*n, W*n)\n",
    "        \n",
    "\n",
    "n_classes = 10\n",
    "n_channels = 3\n",
    "model = UNet(n_channels=n_channels, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterarion:  1, loss = 2.3068\n",
      "51.9413s / iterarion\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=0.99, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "iterarions = 1\n",
    "begin = time.time()\n",
    "\n",
    "# Training\n",
    "for iterarion in range(iterarions):\n",
    "    x = torch.rand(n_channels, 776, 776) # batch == 1\n",
    "    y = torch.randint(0, n_classes, (1, 776, 776))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pred = model(x)\n",
    "    # add dimension of batch for making torchscript happy\n",
    "    loss = criterion(pred.unsqueeze(0), y)\n",
    "\n",
    "    if iterarion % 1 == 0:\n",
    "        print('Iterarion:', '%2d,' % (iterarion + 1), 'loss =', '{:.4f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f\"{(time.time() - begin)/iterarions:.4f}s / iterarion\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

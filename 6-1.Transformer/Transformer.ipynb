{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coded by Lujia Zhong @lujiazho<br>\n",
    "Reference: https://github.com/graykode/nlp-tutorial, https://github.com/Kyubyong/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# positional encoding\n",
    "def sinusoid_pos_encoding(n_position, d_model):\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, (i-i%2)/d_model) for i in range(d_model)] \n",
    "        for pos in range(n_position)])\n",
    "    position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n",
    "    position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n",
    "    return torch.FloatTensor(position_enc)\n",
    "\n",
    "# for encoder & decoder\n",
    "def padding_mask(seq_q, seq_k):\n",
    "    B, len_q = seq_q.shape\n",
    "    B, len_k = seq_k.shape\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # 0 is padding\n",
    "    return pad_attn_mask.expand(B, len_q, len_k)\n",
    "\n",
    "# for decoder\n",
    "def subsequent_mask(seq):\n",
    "    B, len_ = seq.shape\n",
    "    mask = np.triu(np.ones([B, len_, len_]), k=1)\n",
    "    return torch.from_numpy(mask).type(torch.uint8)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.d_key = config.d_key\n",
    "        self.d_value = config.d_value\n",
    "        self.n_heads = config.n_heads\n",
    "        \n",
    "        self.W_Q = nn.Linear(config.d_model, self.d_key * self.n_heads)\n",
    "        self.W_K = nn.Linear(config.d_model, self.d_key * self.n_heads)\n",
    "        self.W_V = nn.Linear(config.d_model, self.d_value * self.n_heads)\n",
    "        \n",
    "        self.linear = nn.Linear(self.n_heads * self.d_value, config.d_model)\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(config.dropout_rate)\n",
    "        self.proj_dropout = nn.Dropout(config.dropout_rate)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # Self-Attn: Q,K,V are torch.Size([2, len_of_sentence, 512])\n",
    "        # Dec-Enc-Attn: Q is torch.Size([2, len_of_dec_sent, 512]) K,V are torch.Size([2, len_of_enc_sent, 512])\n",
    "\n",
    "        residual, batch_size = Q, Q.shape[0]\n",
    "        \n",
    "        query_layer = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_key).transpose(1,2)\n",
    "        key_layer = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_key).transpose(1,2)\n",
    "        value_layer = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_value).transpose(1,2)\n",
    "\n",
    "        # expand in heads' dimension\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
    "        \n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) / np.sqrt(self.d_key)\n",
    "        attention_scores.masked_fill_(attn_mask, -1e9) # masked_fill_: 1 masked, 0 unmasked\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        \n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(1, 2)\n",
    "        context_layer = context_layer.contiguous().view(batch_size, -1, self.n_heads*self.d_value)\n",
    "        \n",
    "        attention_output = self.linear(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        \n",
    "        return self.layer_norm(attention_output + residual), attention_probs\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_mlp)\n",
    "        self.fc2 = nn.Linear(config.d_mlp, config.d_model)\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # torch.Size([2, len_of_sentence, 512])\n",
    "        residual = inputs\n",
    "        \n",
    "        output = nn.ReLU()(self.fc1(inputs))\n",
    "        # torch.Size([2, len_of_sentence, 2048])\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        output = self.fc2(output)\n",
    "        # torch.Size([2, len_of_sentence, 512])\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return self.layer_norm(output + residual)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(config)\n",
    "        self.ffn = MLP(config)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        # torch.Size([2, 4, 512]) torch.Size([2, 4, 4])\n",
    "\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        # torch.Size([2, 4, 512]) torch.Size([2, 8, 4, 4])\n",
    "        \n",
    "        enc_outputs = self.ffn(enc_outputs)\n",
    "        # torch.Size([2, 4, 512])\n",
    "        \n",
    "        return enc_outputs, attn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention(config)\n",
    "        self.dec_enc_attn = MultiHeadAttention(config)\n",
    "        self.ffn = MLP(config)\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        # torch.Size([2, 6, 512]) torch.Size([2, 4, 512]) torch.Size([2, 6, 6]) torch.Size([2, 6, 4])\n",
    "\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        # torch.Size([2, 6, 512]) torch.Size([2, 8, 6, 6])\n",
    "        \n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        # torch.Size([2, 6, 512]) torch.Size([2, 8, 6, 4])\n",
    "        \n",
    "        dec_outputs = self.ffn(dec_outputs)\n",
    "        # torch.Size([2, 6, 512])\n",
    "        \n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, config.d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_pos_encoding(config.src_len+1, config.d_model),\n",
    "                                                    freeze=True)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs):\n",
    "        # torch.Size([2, 4])\n",
    "\n",
    "        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(enc_inputs)\n",
    "        # torch.Size([2, 4, 512])\n",
    "        \n",
    "        enc_self_attn_mask = padding_mask(enc_inputs, enc_inputs)\n",
    "        # torch.Size([2, 4, 4])\n",
    "\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "            # enc_outputs: torch.Size([2, 4, 512])\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        \n",
    "        return enc_outputs, enc_self_attns\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, config.d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_pos_encoding(config.tgt_len+1, config.d_model),\n",
    "                                                    freeze=True)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        # torch.Size([2, 6]) torch.Size([2, 4]) torch.Size([2, 4, 512])\n",
    "\n",
    "        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(dec_inputs)\n",
    "        # torch.Size([2, 6, 512])\n",
    "        \n",
    "        dec_self_attn_pad_mask = padding_mask(dec_inputs, dec_inputs)\n",
    "        # torch.Size([2, 6, 6])\n",
    "        dec_self_attn_subsequent_mask = subsequent_mask(dec_inputs)\n",
    "        # torch.Size([2, 6, 6])\n",
    "        \n",
    "        dec_self_attn_mask = torch.logical_or(dec_self_attn_pad_mask, dec_self_attn_subsequent_mask)\n",
    "        # torch.Size([2, 6, 6])\n",
    "\n",
    "        dec_enc_attn_mask = padding_mask(dec_inputs, enc_inputs)\n",
    "        # torch.Size([2, 6, 4])\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, \n",
    "                                                             dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            # torch.Size([2, 6, 512]) torch.Size([2, 8, 6, 6]) torch.Size([2, 8, 6, 4])\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        \n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "        self.projection = nn.Linear(config.d_model, tgt_vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        # torch.Size([2, 4]) torch.Size([2, 6])\n",
    "\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        # torch.Size([2, 4, 512]) [6, torch.Size([2, 8, 4, 4])]\n",
    "\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        # torch.Size([2, 6, 512]) [6, torch.Size([2, 8, 6, 6])] [6, torch.Size([1, 8, 6, 4])]\n",
    "\n",
    "        logits = self.projection(dec_outputs)\n",
    "        # torch.Size([2, 6, 8])\n",
    "        \n",
    "        return logits.view(-1, logits.shape[-1]), enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "\n",
    "class ModelConfig:\n",
    "    src_len = 4             # length of encoder inputs\n",
    "    tgt_len = 6             # length of decoder inputs/outputs\n",
    "\n",
    "    d_model = 512           # embedding Size\n",
    "    d_mlp = 4*d_model       # MLP hidden dimension\n",
    "    d_key = d_value = 64    # dimension of K == Q, V could be different in dot_product_attention\n",
    "    n_layers = 6            # number of Encoder & Decoder Layer\n",
    "    n_heads = 8             # number of heads in Multi-Head Attention\n",
    "    \n",
    "    dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['quiero una cerveza P', 'quiero una buena cerveza'], \n",
    "             ['S i want a beer P', 'S i want a good beer'], \n",
    "             ['i want a beer P E', 'i want a good beer E']]\n",
    "\n",
    "# S: starting of decoder input\n",
    "# E: ending of decoder output\n",
    "# P: padding will fill in blank sequence for shorter sentence\n",
    "src_vocab = {'P': 0, 'quiero': 1, 'una': 2, 'cerveza': 3, 'buena': 4}\n",
    "idx2src = {i: w for i, w in enumerate(src_vocab)}\n",
    "src_vocab_size = len(src_vocab)\n",
    "\n",
    "tgt_vocab = {'P': 0, 'i': 1, 'want': 2, 'a': 3, 'good': 4, 'beer': 5, 'S': 6, 'E': 7}\n",
    "idx2tgt = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "batch = 2\n",
    "\n",
    "enc_inputs = torch.LongTensor([[src_vocab[n] for n in sentences[0][i].split()] for i in range(batch)])\n",
    "dec_inputs = torch.LongTensor([[tgt_vocab[n] for n in sentences[1][i].split()] for i in range(batch)])\n",
    "target = torch.LongTensor([[tgt_vocab[n] for n in sentences[2][i].split()] for i in range(batch)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0020 cost = 1.810086\n",
      "Epoch: 0040 cost = 1.901784\n",
      "Epoch: 0060 cost = 1.956345\n",
      "Epoch: 0080 cost = 1.947199\n",
      "Epoch: 0100 cost = 1.923884\n",
      "Epoch: 0120 cost = 1.912930\n",
      "Epoch: 0140 cost = 1.919369\n",
      "Epoch: 0160 cost = 1.898509\n",
      "Epoch: 0180 cost = 1.958702\n",
      "Epoch: 0200 cost = 1.897602\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(ModelConfig())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "    loss = criterion(outputs, target.contiguous().view(-1))\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quiero una cerveza P -> i want i a want i\n",
      "quiero una buena cerveza -> beer want a a beer i\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "predict, _, _, _ = model(enc_inputs, dec_inputs)\n",
    "predict = predict.data.max(1, keepdim=True)[1].squeeze().view(batch,-1)\n",
    "for i in range(batch):\n",
    "    print(sentences[0][i], '->', ' '.join([idx2tgt[n.item()] for n in predict[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quiero una cerveza P -> S beer a want want E\n"
     ]
    }
   ],
   "source": [
    "# predicting\n",
    "maximum = 10\n",
    "idx = 0\n",
    "src = enc_inputs[0].unsqueeze(0)\n",
    "pred = torch.tensor([[6]])\n",
    "while (idx < maximum):\n",
    "    predict, _, _, _ = model(src, pred)\n",
    "    nxt = torch.softmax(predict, -1).argmax(-1)[-1]\n",
    "    pred = torch.cat((pred, nxt.unsqueeze(0).unsqueeze(0)), -1)\n",
    "    if (nxt == tgt_vocab['E']):\n",
    "        break\n",
    "    idx += 1\n",
    "    \n",
    "print(' '.join([idx2src[n.item()] for n in src[0]]), \n",
    "      '->', \n",
    "      ' '.join([idx2tgt[n.item()] for n in pred[0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

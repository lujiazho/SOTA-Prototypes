# SOTA-Prototypes

Simple prototypes of SOTAs with dummy data for better vision of model architecture itself.

## Environment
Other versions might work as well.
- Python 3.7.10
- Pytorch 1.11.0

## Prototypes - (Example Purpose)

#### 1. Classification Models

- 1-1. Vision Transformer (ViT)
  - Paper - [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020)](https://arxiv.org/pdf/2010.11929)
  - Ipynb - [ViT.ipynb](1-1.ViT/ViT.ipynb)

- 1-2. Swin Transformer (Swin-T)
  - Paper - [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (2021)](https://arxiv.org/pdf/2103.14030)
  - Ipynb - [Swin-T.ipynb](1-2.Swin-T/Swin-T.ipynb)

- 1-3. Resnet
  - Paper - [Deep Residual Learning for Image Recognition (2015)](https://arxiv.org/pdf/1512.03385)
  - Ipynb - [Resnet.ipynb](1-3.Resnet/Resnet.ipynb)

#### 2. Self-Supervised Models

- 2-1. Masked Autoencoder (MAE)
  - Paper - [Masked Autoencoders Are Scalable Vision Learners (2021)](https://arxiv.org/pdf/2111.06377)
  - Ipynb - [MAE.ipynb](2-1.MAE/MAE.ipynb)

#### 3. Unsupervised Models

- 3-1. Generative Adversarial Networks (GAN)
  - Paper - [Generative Adversarial Networks (2014)](https://arxiv.org/pdf/1406.2661)
  - Ipynb - [GAN.ipynb](3-1.GAN/GAN.ipynb)

#### 4. NLP Models

- 4-1. Transformer
  - Paper - [Attention Is All You Need (2017)](https://arxiv.org/pdf/1706.03762)
  - Ipynb - [Transformer.ipynb](4-1.Transformer/Transformer.ipynb)
  
  
## DistributedDataParallel - (Example Purpose)

- 0-1. Distributed Data Parallel (DDP)
  - Ipynb - [DDP1.0.ipynb](0-1.DDP/DDP1.0.ipynb) (Single Node Single GPU)
  - Ipynb - [DDP2.0.ipynb](0-1.DDP/DDP2.0.ipynb) (Multi Nodes Multi GPUs)
  

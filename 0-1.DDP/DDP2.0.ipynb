{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two steps for training model on Multi GPUs on Multi nodes:\n",
    "1. Initialize this process and join in a group with other processes\n",
    "2. Set gpu number that we want to use for this process\n",
    "3. Send each params that we want to compute with to GPU for this process\n",
    "4. Wrap the model for data parallelism and model sync\n",
    "5. Create data sampler for splitting data for each process with no overlap\n",
    "6. Assign data sampler to DataLoader (Dataloader no need to shuffle because sampler does this by default)\n",
    "\n",
    "Note:\n",
    "1. For each node, we run following script with nr in config from 0 - N-1\n",
    "2. Actual batch_size = batch_size per gpu (25) * world_size (4) = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def main(gpu, args):\n",
    "    # differnet gpu has differnet rank\n",
    "    rank = args.nr * args.g + gpu\n",
    "    # step 1: Initialize this process and join up with the other processes\n",
    "    # No process will continue until all processes have joined\n",
    "    dist.init_process_group(                                   \n",
    "        backend='nccl',              # NVIDIA Collective Communications Library (NCCL) as backend\n",
    "        init_method='env://',        # tells the process group where to look for some settings\n",
    "        world_size=args.world_size,                              \n",
    "        rank=rank\n",
    "    )\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    model = ConvNet()\n",
    "    \n",
    "    # step 2: set gpu number that we want to use\n",
    "    torch.cuda.set_device(gpu)\n",
    "    \n",
    "    # step 3: send each params that we want to compute with to GPU\n",
    "    model.cuda(gpu)\n",
    "    \n",
    "    # per gpu 25, so in total we actually run a batch_size of 25 * world_size\n",
    "    batch_size = 25\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 1e-4)\n",
    "    \n",
    "    # step 4: Wrap the model, ensuring data parallelism and backwards gradients are averaged\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[gpu])\n",
    "    \n",
    "    # Data loading code\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                               train=True,\n",
    "                                               transform=transforms.ToTensor(),\n",
    "                                               download=True)\n",
    "    # step 5: makes sure that each process gets a different slice of the training data\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset,\n",
    "        num_replicas=args.world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True,\n",
    "                                               sampler=train_sampler)\n",
    "\n",
    "    start = datetime.now()\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(args.epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i + 1) % 100 == 0 and gpu == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
    "                    epoch + 1, \n",
    "                    args.epochs, \n",
    "                    i + 1, \n",
    "                    total_step,\n",
    "                    loss.item())\n",
    "                   )\n",
    "    if gpu == 0:\n",
    "        print(\"Training complete in: \" + str(datetime.now() - start))\n",
    "\n",
    "class config:\n",
    "    n = 1   # number of nodes\n",
    "    g = 4   # number of gpus per node\n",
    "    nr = 0  # the rank of the current node within all the nodes\n",
    "    epochs = 1\n",
    "    \n",
    "    world_size = g*n  # total number of gpus == total number of processes\n",
    "    master_addr = '10.7.3.62'\n",
    "    master_port = '8888'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = config()\n",
    "    os.environ['MASTER_ADDR'] = args.master_addr\n",
    "    os.environ['MASTER_PORT'] = args.master_port\n",
    "\n",
    "    # start process for each gpu\n",
    "    mp.spawn(main, nprocs=args.g, args=(args,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

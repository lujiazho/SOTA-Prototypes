{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coded by Lujia Zhong @lujiazho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import MSELoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
    "        self.attention_head_size = int(config.embedding_size / self.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = Linear(config.embedding_size, self.all_head_size)\n",
    "        self.key = Linear(config.embedding_size, self.all_head_size)\n",
    "        self.value = Linear(config.embedding_size, self.all_head_size)\n",
    "\n",
    "        self.out = Linear(config.embedding_size, config.embedding_size)\n",
    "        self.attn_dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "        self.proj_dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):        \n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / (self.attention_head_size**0.5)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        weights = attention_probs\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        \n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        \n",
    "        return attention_output, weights\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = Linear(config.embedding_size, config.transformer[\"mlp_dim\"])\n",
    "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.embedding_size)\n",
    "        self.act_fn = torch.nn.functional.gelu\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        self.embedding_size = config.embedding_size\n",
    "        self.attention_norm = LayerNorm(config.embedding_size, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(config.embedding_size, eps=1e-6)\n",
    "        self.ffn = MLP(config)\n",
    "        self.attn = Attention(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        x, weights = self.attn(x)\n",
    "        \n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        x = x + h\n",
    "        return x, weights\n",
    "\n",
    "class Coder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Coder, self).__init__()\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.coder_norm = LayerNorm(config.embedding_size, eps=1e-6)\n",
    "        for _ in range(config.transformer[\"num_layers\"]):\n",
    "            layer = Block(config)\n",
    "            self.layer.append(layer)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: torch.Size([4, 65, 768])\n",
    "        \n",
    "        attn_weights = []\n",
    "        for layer_block in self.layer:\n",
    "            hidden_states, weights = layer_block(hidden_states)\n",
    "            attn_weights.append(weights)\n",
    "        # hidden_states: torch.Size([4, 65, 768])\n",
    "        \n",
    "        encoded = self.coder_norm(hidden_states)\n",
    "        # encoded: torch.Size([4, 65, 768])\n",
    "        \n",
    "        return encoded, attn_weights\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config, position_embeddings, img_size, in_channels=3):\n",
    "        super(Embeddings, self).__init__()\n",
    "        img_size = (img_size, img_size)\n",
    "\n",
    "        patch_size = (config.patch_size, config.patch_size)\n",
    "        embedding_size = config.enc_config.embedding_size\n",
    "\n",
    "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
    "                                       out_channels=embedding_size,\n",
    "                                       kernel_size=patch_size,\n",
    "                                       stride=patch_size)\n",
    "        self.position_embeddings = position_embeddings[:,1:,:]\n",
    "        # self.position_embeddings: torch.Size([1, 256, 768])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: torch.Size([4, 3, 256, 256])\n",
    "        \n",
    "        x = self.patch_embeddings(x)\n",
    "        # x: torch.Size([4, 768, 16, 16])\n",
    "        x = x.flatten(2)\n",
    "        # x: torch.Size([4, 768, 256])\n",
    "        x = x.transpose(-1, -2)\n",
    "        # x: torch.Size([4, 256, 768])\n",
    "        \n",
    "        embeddings = x + self.position_embeddings\n",
    "        # embeddings: torch.Size([4, 256, 768])\n",
    "        return embeddings\n",
    "\n",
    "class MAE(nn.Module):\n",
    "    def __init__(self, config, img_size=256, in_channels=3):\n",
    "        super().__init__()\n",
    "        # common\n",
    "        assert 0 <= config.masking_ratio < 1, 'masking ratio must be kept between 0 and 1'\n",
    "        self.in_c = in_channels\n",
    "        self.masking_ratio = config.masking_ratio\n",
    "        self.patch_size = config.patch_size\n",
    "        self.n_patches = (img_size // self.patch_size) * (img_size // self.patch_size)\n",
    "        \n",
    "        encoder_dim = config.enc_config.embedding_size\n",
    "        decoder_dim = config.dec_config.embedding_size\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, encoder_dim))\n",
    "        # self.cls_token: torch.Size([1, 1, 768])\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, self.n_patches+1, encoder_dim))\n",
    "\n",
    "        self.embeddings = Embeddings(config, self.position_embeddings, img_size=img_size)\n",
    "        self.encoder = Coder(config.enc_config)\n",
    "\n",
    "        # encoder 2 decoder\n",
    "        self.enc_to_dec = nn.Linear(encoder_dim, decoder_dim) if encoder_dim != decoder_dim else nn.Identity()\n",
    "        \n",
    "        self.mask_token = nn.Parameter(torch.randn(decoder_dim))\n",
    "        self.decoder = Coder(config.dec_config)\n",
    "\n",
    "        self.decoder_pos_emb = nn.Parameter(torch.zeros(1, self.n_patches+1, decoder_dim))\n",
    "        self.to_pixels = nn.Linear(decoder_dim, self.patch_size*self.patch_size*self.in_c)\n",
    "        # loss\n",
    "        self.criterion = MSELoss()\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        # imgs: [4, 3, 256, 256]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % self.patch_size == 0\n",
    "\n",
    "        p_side = imgs.shape[2] // self.patch_size\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], self.in_c, p_side, self.patch_size, p_side, self.patch_size))\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1)\n",
    "        x = x.reshape(shape=(imgs.shape[0], p_side * p_side, self.patch_size**2 * self.in_c))\n",
    "        # x: [4, 256, 768]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def unpatchify(self, x):\n",
    "        # x: [4, 256, 768]\n",
    "        p_side = int(x.shape[1]**.5)\n",
    "        assert p_side**2 == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], p_side, p_side, self.patch_size, self.patch_size, self.in_c))\n",
    "        x = x.permute(0, 5, 1, 3, 2, 4)\n",
    "        imgs = x.reshape(shape=(x.shape[0], self.in_c, p_side * self.patch_size, p_side * self.patch_size))\n",
    "        # imgs: [4, 3, 256, 256]\n",
    "        \n",
    "        return imgs\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        # img: torch.Size([4, 3, 256, 256])\n",
    "        batch = imgs.shape[0]\n",
    "        \n",
    "        # encoder embeddings\n",
    "        embedding_output = self.embeddings(imgs)\n",
    "        # tokens: torch.Size([4, 256, 768])\n",
    "\n",
    "        # shuffle\n",
    "        num_masked = int(self.masking_ratio * self.n_patches)            # num_masked: 192\n",
    "        rand_indices = torch.rand(batch, self.n_patches).argsort(dim=-1) # torch.Size([4, 256])\n",
    "        masked_indices, unmasked_indices = rand_indices[:, :num_masked], rand_indices[:, num_masked:]\n",
    "        # masked_indices: torch.Size([4, 192])\n",
    "        # unmasked_indices: torch.Size([4, 64])\n",
    "        batch_range = torch.arange(batch)[:, None]                      # tensor([[0], [1], [2], [3]])\n",
    "        \n",
    "        # tokens part\n",
    "        tokens = embedding_output[batch_range, unmasked_indices]        # torch.Size([4, 64, 768])\n",
    "        \n",
    "        # add cls token\n",
    "        cls_tokens = self.cls_token.expand(batch, -1, -1)               # torch.Size([4, 1, 768])\n",
    "        cls_tokens = cls_tokens + self.position_embeddings[:,:1,:]      # torch.Size([4, 1, 768])\n",
    "        tokens = torch.cat((cls_tokens, tokens), dim=1)                 # torch.Size([4, 65, 768])\n",
    "        \n",
    "        # encoder\n",
    "        encoded_tokens, enc_attns = self.encoder(tokens)                # torch.Size([4, 65, 768])\n",
    "        \n",
    "        # decoder embeddings\n",
    "        decoder_tokens = self.enc_to_dec(encoded_tokens)                # torch.Size([4, 65, 512])\n",
    "        cls_tokens = decoder_tokens[:,:1,:]                             # torch.Size([4, 1, 512])\n",
    "        decoder_tokens = decoder_tokens[:,1:,:]                         # torch.Size([4, 64, 512])\n",
    "        \n",
    "        # masked learnable tokens\n",
    "        mask_tokens = self.mask_token.repeat(batch, num_masked, 1)           # torch.Size([4, 192, 512])\n",
    "        decoder_tokens = torch.cat((mask_tokens, decoder_tokens), dim=1)     # torch.Size([4, 256, 512])\n",
    "        # unshuffle\n",
    "        decoder_tokens = decoder_tokens[batch_range, rand_indices.sort()[1]] # torch.Size([4, 256, 512])\n",
    "        # add cls token\n",
    "        decoder_tokens = torch.cat((cls_tokens, decoder_tokens), dim=1)      # torch.Size([4, 257, 512])\n",
    "        mask_tokens = decoder_tokens + self.decoder_pos_emb                  # torch.Size([4, 257, 512])\n",
    "\n",
    "        # decoder\n",
    "        decoded_tokens, dec_attns = self.decoder(decoder_tokens)        # torch.Size([4, 256, 512])\n",
    "\n",
    "        # only take the masked part as pred\n",
    "        mask_tokens = decoded_tokens[batch_range, masked_indices]       # torch.Size([4, 192, 512])\n",
    "        pred_pixel_values = self.to_pixels(mask_tokens)                 # torch.Size([4, 192, 768])\n",
    "\n",
    "        # loss\n",
    "        pateched_imgs = self.patchify(imgs)\n",
    "        loss = self.criterion(pred_pixel_values, pateched_imgs[batch_range, masked_indices])\n",
    "        \n",
    "        # predicted img\n",
    "        imgs = torch.cat((pred_pixel_values, \n",
    "                          pateched_imgs[batch_range, unmasked_indices]), dim=1)    # torch.Size([4, 256, 768])\n",
    "        imgs = imgs[batch_range, rand_indices.sort()[1]]                           # torch.Size([4, 256, 768])\n",
    "        pred_imgs = self.unpatchify(imgs)\n",
    "        return loss, pred_imgs, enc_attns, dec_attns\n",
    "    \n",
    "class ModelConfig:\n",
    "    patch_size = 16\n",
    "    masking_ratio = 0.75\n",
    "    \n",
    "    # encoder\n",
    "    class enc_config:\n",
    "        embedding_size = 768\n",
    "        transformer = {\n",
    "            'mlp_dim': 3072,\n",
    "            'num_heads': 12,\n",
    "            'num_layers': 12,\n",
    "            'dropout_rate': 0.0\n",
    "        }\n",
    "    \n",
    "    # decoder\n",
    "    class dec_config:\n",
    "        embedding_size = 512\n",
    "        transformer = {\n",
    "            'mlp_dim': 2048,\n",
    "            'num_heads': 8,\n",
    "            'num_layers': 8,\n",
    "            'dropout_rate': 0.0\n",
    "        }\n",
    "\n",
    "img_size = 256\n",
    "model = MAE(ModelConfig(), img_size=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterarion:  1, loss = 1.3392\n",
      "Iterarion:  2, loss = 1.2834\n",
      "Iterarion:  3, loss = 1.2110\n",
      "Iterarion:  4, loss = 1.1590\n",
      "Iterarion:  5, loss = 1.1276\n",
      "2.5700s / iterarion\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=3e-2,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=0)\n",
    "iterarions = 5\n",
    "begin = time.time()\n",
    "# Training\n",
    "for iterarion in range(iterarions):\n",
    "    x = torch.Tensor(np.random.randn(4, 3, img_size, img_size))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss, *_ = model(x)\n",
    "    \n",
    "    if iterarion % 1 == 0:\n",
    "        print('Iterarion:', '%2d,' % (iterarion + 1), 'loss =', '{:.4f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f\"{(time.time() - begin)/iterarions:.4f}s / iterarion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting\n",
    "x = torch.randn(4, 3, img_size, img_size)\n",
    "loss, pred, *_ = model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coded by Lujia Zhong @lujiazho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
    "        self.attention_head_size = int(config.embedding_size / self.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = Linear(config.embedding_size, self.all_head_size)\n",
    "        self.key = Linear(config.embedding_size, self.all_head_size)\n",
    "        self.value = Linear(config.embedding_size, self.all_head_size)\n",
    "\n",
    "        self.out = Linear(config.embedding_size, config.embedding_size)\n",
    "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: torch.Size([4, 197, 768])\n",
    "        \n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        # mixed_query_layer: torch.Size([4, 197, 768])\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        # mixed_key_layer: torch.Size([4, 197, 768])\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        # mixed_value_layer: torch.Size([4, 197, 768])\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        # query_layer: torch.Size([4, 12, 197, 64])\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        # key_layer: torch.Size([4, 12, 197, 64])\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "        # value_layer: torch.Size([4, 12, 197, 64])\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        # attention_scores: torch.Size([4, 12, 197, 197])\n",
    "        attention_scores = attention_scores / (self.attention_head_size**0.5)\n",
    "        # attention_scores: torch.Size([4, 12, 197, 197])\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        # attention_probs: torch.Size([4, 12, 197, 197])\n",
    "        weights = attention_probs\n",
    "        # weights: None\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        # attention_probs: torch.Size([4, 12, 197, 197])\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        # context_layer: torch.Size([4, 12, 197, 64])\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        # context_layer: torch.Size([4, 197, 12, 64])\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        \n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        # context_layer: torch.Size([4, 197, 768])\n",
    "        attention_output = self.out(context_layer)\n",
    "        # attention_output: torch.Size([4, 197, 768])\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        # attention_output: torch.Size([4, 197, 768])\n",
    "        \n",
    "        return attention_output, weights\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = Linear(config.embedding_size, config.transformer[\"mlp_dim\"])\n",
    "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.embedding_size)\n",
    "        self.act_fn = torch.nn.functional.gelu\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: torch.Size([4, 197, 768])\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        # x: torch.Size([4, 197, 3072])\n",
    "        \n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # x: torch.Size([4, 197, 768])\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config, img_size, in_channels=3):\n",
    "        super(Embeddings, self).__init__()\n",
    "        img_size = (img_size, img_size)\n",
    "\n",
    "        patch_size = config.patches\n",
    "        n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "\n",
    "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
    "                                       out_channels=config.embedding_size,\n",
    "                                       kernel_size=patch_size,\n",
    "                                       stride=patch_size)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.embedding_size))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.embedding_size))\n",
    "        # self.cls_token: torch.Size([1, 1, 768])\n",
    "\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: torch.Size([4, 3, 224, 224])\n",
    "        \n",
    "        B = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        # cls_tokens: torch.Size([4, 1, 768])\n",
    "\n",
    "        x = self.patch_embeddings(x)\n",
    "        # x: torch.Size([4, 768, 14, 14])\n",
    "        x = x.flatten(2)\n",
    "        # x: torch.Size([4, 768, 196])\n",
    "        x = x.transpose(-1, -2)\n",
    "        # x: torch.Size([4, 196, 768])\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        # x: torch.Size([4, 197, 768])\n",
    "\n",
    "        embeddings = x + self.position_embeddings\n",
    "        # embeddings: torch.Size([4, 197, 768])\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        self.embedding_size = config.embedding_size\n",
    "        self.attention_norm = LayerNorm(config.embedding_size, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(config.embedding_size, eps=1e-6)\n",
    "        self.ffn = MLP(config)\n",
    "        self.attn = Attention(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: torch.Size([4, 197, 768])\n",
    "        \n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        x, weights = self.attn(x)\n",
    "        # x: torch.Size([4, 197, 768])\n",
    "        \n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        # x: torch.Size([4, 197, 768])\n",
    "        \n",
    "        x = x + h\n",
    "        return x, weights\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(config.embedding_size, eps=1e-6)\n",
    "        for _ in range(config.transformer[\"num_layers\"]):\n",
    "            layer = Block(config)\n",
    "            self.layer.append(layer)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: torch.Size([4, 197, 768])\n",
    "        \n",
    "        attn_weights = []\n",
    "        for layer_block in self.layer:\n",
    "            hidden_states, weights = layer_block(hidden_states)\n",
    "            attn_weights.append(weights)\n",
    "        # hidden_states: torch.Size([4, 197, 768])\n",
    "        \n",
    "        encoded = self.encoder_norm(hidden_states)\n",
    "        # encoded: torch.Size([4, 197, 768])\n",
    "        \n",
    "        return encoded, attn_weights\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config, img_size=224, num_classes=21843):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.embeddings = Embeddings(config, img_size=img_size)\n",
    "        self.encoder = Encoder(config)\n",
    "        self.head = Linear(config.embedding_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: torch.Size([4, 3, 224, 224])\n",
    "        \n",
    "        embedding_output = self.embeddings(x)\n",
    "        # embedding_output: torch.Size([4, 197, 768])\n",
    "        \n",
    "        encoded, attn_weights = self.encoder(embedding_output)\n",
    "        # encoded: torch.Size([4, 197, 768])\n",
    "        # attn_weights: [12, torch.Size([4, 12, 197, 197])]\n",
    "        \n",
    "        logits = self.head(encoded[:, 0]) # this is cls, or use np.mean(x, axis=1)\n",
    "        # logits: torch.Size([4, 10])\n",
    "\n",
    "        return logits, attn_weights\n",
    "\n",
    "class ModelConfig:\n",
    "    patches = (16, 16)\n",
    "    embedding_size = 768\n",
    "    transformer = {\n",
    "        'mlp_dim': 3072,\n",
    "        'num_heads': 12,\n",
    "        'num_layers': 12,\n",
    "        'attention_dropout_rate': 0.0,\n",
    "        'dropout_rate': 0.1\n",
    "    }\n",
    "\n",
    "img_size = 224\n",
    "model = VisionTransformer(ModelConfig(), img_size=img_size, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterarion:  1, loss = 2.4564\n",
      "Iterarion:  2, loss = 1.7643\n",
      "3.7500s / iterarion\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=3e-2,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=0)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "iterarions = 2\n",
    "begin = time.time()\n",
    "# Training\n",
    "for iterarion in range(iterarions):\n",
    "    x = torch.Tensor(np.random.randn(4, 3, img_size, img_size))\n",
    "    y = torch.LongTensor([0,1,2,3])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pred, atten_weights = model(x)\n",
    "    loss = criterion(pred, y)\n",
    "\n",
    "    if iterarion % 1 == 0:\n",
    "        print('Iterarion:', '%2d,' % (iterarion + 1), 'loss =', '{:.4f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f\"{(time.time() - begin)/iterarions:.4f}s / iterarion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting\n",
    "pred, atten = model(torch.randn(4, 3, img_size, img_size))\n",
    "pred.data.max(1, keepdim=True)[1].squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
